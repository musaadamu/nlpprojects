{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision transformers pandas gradio\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50\n",
    "from transformers import BertTokenizer, BertModel, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gradio as gr\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/nlp_colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Load the data\n",
    "data = pd.read_csv('data2.csv')  # Path to your CSV file\n",
    "\n",
    "# Define image transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def load_image(image_id, image_folder='images'):\n",
    "    image_path = os.path.join(image_folder, f\"{image_id}.png\")  # Adjust extension if needed\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"FileNotFoundError: Image {image_path} not found.\")\n",
    "        raise FileNotFoundError(f\"Image {image_path} not found.\")\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    return transform(image)\n",
    "\n",
    "# Initialize lists to store images, questions, and labels\n",
    "images = []\n",
    "questions = []\n",
    "labels = []\n",
    "\n",
    "# Create label mapping\n",
    "unique_labels = data['answer'].unique()\n",
    "label_map = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(data['answer'])\n",
    "\n",
    "# Load images, questions, and labels\n",
    "for idx, row in data.iterrows():\n",
    "    try:\n",
    "        # Load and transform the image\n",
    "        image_tensor = load_image(row['image_id'], image_folder='images')\n",
    "        images.append(image_tensor)\n",
    "        \n",
    "        # Concatenate questions\n",
    "        combined_question = \" \".join([row['englishquestion'], row['hausaquestion'], row['englishhausaquestion']])\n",
    "        questions.append(combined_question)\n",
    "        \n",
    "        labels.append(label_map[row['answer']])\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "        continue\n",
    "\n",
    "# Tokenize questions\n",
    "question_tokens = tokenizer(questions, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(\"Questions tokenized shape:\", question_tokens['input_ids'].shape)\n",
    "\n",
    "# Convert labels to a tensor\n",
    "label_tensor = torch.tensor(labels)\n",
    "print(\"Labels tensor shape:\", label_tensor.shape)\n",
    "\n",
    "# Stack images into a single tensor\n",
    "image_stack = torch.stack(images)\n",
    "print(\"Images stacked shape:\", image_stack.shape)\n",
    "\n",
    "print(\"Data preparation successful.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, question_tokens, images, labels):\n",
    "        self.input_ids = question_tokens['input_ids']\n",
    "        self.attention_mask = question_tokens['attention_mask']\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attention_mask[idx], self.images[idx], self.labels[idx]\n",
    "\n",
    "# Create dataset\n",
    "dataset = CustomDataset(question_tokens, image_stack, label_tensor)\n",
    "\n",
    "# Split dataset into train and eval sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "eval_size = len(dataset) - train_size\n",
    "train_dataset, eval_dataset = random_split(dataset, [train_size, eval_size])\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BertCNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertCNNModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(16 * 112 * 112 + 768, 512)\n",
    "        self.fc2 = nn.Linear(512, len(label_map))\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, images):\n",
    "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        bert_pooled_output = bert_outputs.pooler_output\n",
    "        x = self.pool(F.relu(self.conv1(images)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        combined = torch.cat((x, bert_pooled_output), dim=1)\n",
    "        x = F.relu(self.fc1(combined))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BertCNNModel().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize loss function and optimizer\n",
    "loss_fn = CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):  # Adjust number of epochs as needed\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        input_ids, attention_mask, images, labels = [x.to(device) for x in batch]\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask, images)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    eval_preds = []\n",
    "    eval_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_dataloader:\n",
    "            input_ids, attention_mask, images, labels = [x.to(device) for x in batch]\n",
    "            outputs = model(input_ids, attention_mask, images)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            eval_preds.extend(preds.cpu().numpy())\n",
    "            eval_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(eval_labels, eval_preds)\n",
    "    print(f\"Epoch {epoch+1}, Evaluation Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def predict(question, image):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Preprocess input\n",
    "        question_tokens = tokenizer(question, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Make prediction\n",
    "        outputs = model(question_tokens['input_ids'], question_tokens['attention_mask'], image_tensor)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        prediction = label_encoder.inverse_transform(preds.cpu().numpy())\n",
    "        \n",
    "    return prediction[0]\n",
    "\n",
    "# Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=predict,\n",
    "    inputs=[\n",
    "        gr.Textbox(lines=2, placeholder=\"Enter the question here...\"),\n",
    "        gr.Image(type=\"pil\")\n",
    "    ],\n",
    "    outputs=\"text\"\n",
    ")\n",
    "\n",
    "iface.launch(share=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arewaDS_functions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
